{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINMA 2472 : Algorithms in Data Science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project on deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a common task in machine learning.  In this project, we will tackle the task of classifying tweets of the two presidential candidates for the 2017 election.  To do so, we will use a database of about 20k tweets of both candidates.  For Donald Trump, we have all the tweets he posted from 05/04/2009 to 11/26/2017 while we have for Hillary Clinton her tweets from 06/10/2013 to 11/24/2017.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project, we decided to use the python version of the library [TensorFlow](https://www.tensorflow.org/).  Since this library is quite powerful, we decided not to use its high level tools as a black box and code ourselves our classifier as much as possible.  One could argue that we could develop our classifier without the help of any library, but we could not have the same results for sure.  Backpropagation may be quite tricky to implement and coding a fancier optimization method than Gradient method would have been out of range considering the time resources we had.  Moreover, TensorFlow provides us great tools to track the performance of our classifiers as the TensorBoard tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developped a python abstraction for classifiers you can find in classifier.py.  In that way, it is really easy to build another classifier based on another model, you just have to redefine the method create_model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hdev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import csv\n",
    "\n",
    "# Our custom libraries\n",
    "from nlp_utils import generate_bow, create_bow_by_dict\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    training_list = []\n",
    "    label_list = []\n",
    "    file = open(filename, \"r\")\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    for tweet, author in reader:\n",
    "        training_list.append(tweet)\n",
    "        label_list.append(author)\n",
    "    file.close()\n",
    "    \n",
    "    return {'x' : training_list, 'label' : label_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork(Classifier):\n",
    "    \n",
    "    def create_model(self, hidden_layers):\n",
    "        # At least one hidden layer !\n",
    "        # Create the structure of the deep neural network\n",
    "       \n",
    "        input_layer_size = len(self.train_set['x'][0]) \n",
    "        output_layer_size = self.nb_classes\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_layer_size], name='input')\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, output_layer_size], name = 'label')\n",
    "        \n",
    "        self.hidden_layers = hidden_layers \n",
    "\n",
    "        self.weights = []\n",
    "        self.layers = []\n",
    "        self.bias = []\n",
    "        \n",
    "        W = tf.Variable(tf.random_normal([input_layer_size, self.hidden_layers[0]], stddev=0.01))\n",
    "        self.weights.append(W) \n",
    "        b = tf.Variable(tf.zeros([self.hidden_layers[0]]))\n",
    "        self.bias.append(b)\n",
    "        y = tf.nn.sigmoid(tf.matmul(self.x, W)+b)\n",
    "        self.layers.append(y)\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            W = tf.Variable(tf.random_normal([self.hidden_layers[i], self.hidden_layers[i+1]], stddev=0.1))\n",
    "            self.weights.append(W) \n",
    "            b = tf.Variable(tf.zeros([self.hidden_layers[i+1]]))\n",
    "            self.bias.append(b)\n",
    "            y = tf.nn.sigmoid(tf.matmul(self.layers[i], W)+b)\n",
    "            self.layers.append(y)\n",
    "\n",
    "        W = tf.Variable(tf.random_normal([self.hidden_layers[-1], output_layer_size], stddev=0.1))\n",
    "        self.weights.append(W)\n",
    "        b = tf.Variable(tf.zeros([output_layer_size]))\n",
    "        self.bias.append(b)\n",
    "        \n",
    "        self.y = tf.matmul(self.layers[-1], W)+b\n",
    "\n",
    "        # Loss function\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "        # Training accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        self.training_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Optimizer\n",
    "        self.train_step = tf.train.AdamOptimizer().minimize(self.loss, global_step = self.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_raw = read_files('training.csv')\n",
    "test_set_raw = read_files('test.csv')\n",
    "\n",
    "\n",
    "train_set_features , word_dict = generate_bow(train_set_raw['x'], train_set_raw['label'], True)\n",
    "test_set_features = create_bow_by_dict(test_set_raw['x'], word_dict, True)\n",
    "\n",
    "\n",
    "train_set = {'x' : train_set_features, 'label' : [np.array([int(s == 'Trump'),1-int(s == 'Trump')]) for s in train_set_raw['label']]}\n",
    "test_set = {'x' : test_set_features, 'label' : [np.array([int(s == 'Trump'),1-int(s == 'Trump')]) for s in test_set_raw['label']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Training step :\n",
      "accuracy : 0.873621568535 mean loss)\n"
     ]
    }
   ],
   "source": [
    "# example : \n",
    "tf.reset_default_graph() \n",
    "\n",
    "DNN = DeepNeuralNetwork(train_set, test_set, 2, name='6_layers_perceptron')\n",
    "DNN.create_model([16, 16, 16, 16])\n",
    "DNN.run()\n",
    "print(DNN.sess.run(DNN.bias[0], feed_dict= {DNN.x: [test_set['x'][0]]}))\n",
    "DNN.train(10)\n",
    "#DNN.restore()\n",
    "print('accuracy : ' + str(DNN.test()))\n",
    "DNN.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we test different deep-learning architectures with the library Keras.\n",
    "\n",
    "The tested architectures are convolutionnals one based on this article \"Convolutional Neural Networks for Sentence Classification [2014]\" by Yoon Kim. Where a precomputed embedding of the words done by Word2Vec on google-news data is used.\n",
    "\n",
    "We also test a two stacked LSTM architecture, still using the word embeddings.\n",
    "\n",
    "Plot of the different models by Keras can be found in the \"model#.png\" files.\n",
    "\n",
    "To run this part you will need :\n",
    "    -numpy\n",
    "    for the embedding :\n",
    "        -gensim\n",
    "        -Word2Vec google-news embedding, it can be found here https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit \n",
    "    \n",
    "    For model formulation and optimization:\n",
    "    -keras\n",
    "    For model visualization, given by \"model#.png\" files (optionnal decomment \"plot_model()\" call if you need it):\n",
    "    -pydot\n",
    "    -graphviz (apt-get graphviz, not the anaconda package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import gc\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "if not 'embeddingModel' in vars():\n",
    "    embeddingModel = 0\n",
    "    gc.collect()\n",
    "    embeddingModel = KeyedVectors.load_word2vec_format(os.environ['HOME']+'/Documents/Word2Vec_embedding/GoogleNews-vectors-negative300.bin', binary=True)#, norm_only=True)\n",
    "\n",
    "def embedding(tweet):\n",
    "    \"\"\" convert a tweet to a matrix\n",
    "        with the embedding from Word2Vec to GoogleNews\n",
    "    \"\"\"\n",
    "    E = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if word in embeddingModel:\n",
    "            E.append(embeddingModel[word])\n",
    "    \n",
    "    return np.array(E)\n",
    "        \n",
    "\n",
    "def create_dataset(filename):\n",
    "    training_list = []\n",
    "    label_list = []\n",
    "    file = open(filename, \"r\")\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    for tweet, author in reader:\n",
    "        E = embedding(tweet)\n",
    "        if not E.size<3*300:\n",
    "            training_list.append(E)\n",
    "            label_list.append(int(author=='Trump'))\n",
    "    file.close()\n",
    "\n",
    "    return {'x': training_list, 'label': label_list}\n",
    "\n",
    "Train_dataset = create_dataset('training.csv')\n",
    "x_train = Train_dataset['x']\n",
    "y_train = Train_dataset['label']\n",
    "\n",
    "Test_dataset = create_dataset('test.csv')\n",
    "x_test = Test_dataset['x']\n",
    "y_test = Test_dataset['label']\n",
    "\n",
    "#what is the length of the maximal sequence of words (for padding)\n",
    "seq_length = max(max([x.shape[0] for x in x_train]), max([x.shape[0] for x in x_test]))\n",
    "\n",
    "def zero_padding(X):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = np.vstack((X[i], np.zeros((seq_length-X[i].shape[0],300))))\n",
    "\n",
    "zero_padding(x_train)\n",
    "zero_padding(x_test)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model\n",
    "A first simpler implementation of the one given in the article. With only one convolutionnal kernel size (3) with 128 features, a global max pooling layer and a fully connected layer to the one node output.\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(128, 3, activation='relu', input_shape=(seq_length,300), name=\"Convolution\"))\n",
    "model.add(GlobalMaxPooling1D(name=\"Pooling\"))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model1.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a convolutionnal layer with different kernel sizes\n",
    "A component of the two following models. Implement one convolutionnal layer with three kernel sizes (3,4,5) 100 features each and global max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_length,300), name=\"Convolution_Input\")\n",
    "convs = []\n",
    "#1\n",
    "conv = Conv1D(100, 3, activation='relu', name=\"Convolution_Ker_Size3\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling1\")(conv)\n",
    "convs.append(pool)\n",
    "#2\n",
    "conv = Conv1D(100, 4, activation='relu', name=\"Convolution_Ker_Size4\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling2\")(conv)\n",
    "convs.append(pool)\n",
    "#3\n",
    "conv = Conv1D(100, 5, activation='relu', name=\"Convolution_Ker_Size5\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling3\")(conv)\n",
    "convs.append(pool)\n",
    "out = Concatenate(name=\"Merge\")(convs)\n",
    "\n",
    "conv_model = Model(inputs=inp, outputs=out)\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model\n",
    "Close to the model presented in the article. The three kernel size for the convolutionnal layer, Dropout on the hidden layer with p=0.5, and a l2 loss on the last matrix weights (l2 constraint in the article).\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(conv_model)\n",
    "model.add(Dropout(0.5, name=\"Dropout\"))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01), name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model2.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third model\n",
    "A 20 nodes fully connected intermediate layer is added before the output.\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(conv_model)\n",
    "model.add(Dropout(0.5, name=\"Dropout\"))\n",
    "model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.01), name=\"Intermediate_Dense\"))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01), name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model3.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth model\n",
    "Two stacked LSTM.\n",
    "The first as a 64 dimensionnal state and return it at each time step (word). The second as a 32 dimensionnal state and only return it at the end. This last state is then used to compute the output using a dense layer.\n",
    "\n",
    "Observed test set accuracy : ~90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,input_shape=(seq_length,300), name=\"First_Stacked_LSTM\"))\n",
    "model.add(LSTM(32, name=\"Second_Stacked_LSTM\"))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=20, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model4.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "There is no big difference of performance between the models. Training time where roughly the same ~1-2min.\n",
    "\n",
    "Interestingly the accuracy on the training set where usually far better than the accuracy on the test set for the convolutionnal models. But this was not observed for the two stacked LSTM.\n",
    "\n",
    "More than 90% accuracy seems acceptable since the model works on the semantic of the words used, rather than the syntax due to the embedding (assuming the embedding reflects the semantic)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
