{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LINMA 2472 : Algorithms in Data Science**\n",
    "===============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project on deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a common task in machine learning.  In this project, we will tackle the task of classifying tweets of the two presidential candidates for the 2017 election.  To do so, we will use a database of about 20k tweets of both candidates.  For Donald Trump, we have all the tweets he posted from 05/04/2009 to 11/26/2017 while we have for Hillary Clinton her tweets from 06/10/2013 to 11/24/2017.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project, we decided to use the python version of the library [TensorFlow](https://www.tensorflow.org/).  Since this library is quite powerful, we decided not to use its high level tools as a black box and code ourselves our classifier as much as possible.  One could argue that we could develop our classifier without the help of any library, but we could not have the same results for sure.  Backpropagation may be quite tricky to implement and coding a fancier optimization method than Gradient method would have been out of range considering the time resources we had.  Moreover, TensorFlow provides us great tools to track the performance of our classifiers as the TensorBoard tool.  In this first part, we implemented a simple perceptron model with a naïve feature extraction of the texts : a bag of word representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used Keras. Keras is a higher order library based on one lower level library, like TensorFlow or CNTK for example. We used it to easily test several models, convolutionnals and recurrent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developped a python abstraction for classifiers you can find in classifier.py.  In that way, it is really easy to build another classifier based on another model, you just have to redefine the method create_model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you need to lauch this part of the project :\n",
    "\n",
    " - numpy\n",
    " - nltk (For the bag of words processing)\n",
    " - [Tensorflow](https://www.tensorflow.org/install/)\n",
    "\n",
    "You can track the progress of the training by launching tensorboard in your terminal :\n",
    "`tensorboard --logdirs='.graphs/'`\n",
    "Then, open the link provided in your web browser to use the tools of tensorboard.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hdev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import csv\n",
    "\n",
    "# Our custom libraries\n",
    "from nlp_utils import generate_bow, create_bow_by_dict\n",
    "from utils import read_files\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PerceptronClassifier(Classifier):\n",
    "    # This class inherit from the the class Classifier, an abstraction dealing with the run of the flows, the saving of \n",
    "    # the  intermediate results, the training step, the testing step, ...\n",
    "    # See classifier.py for more informations\n",
    "    \n",
    "    def create_model(self, hidden_layers):\n",
    "        # Create the structure of the perceptron.\n",
    "        # hidden_layers should be a list containing the number of nodes for each layer.\n",
    "        # Each layer is densely connected with the previous and next one.\n",
    "        # The number of layer can be arbitrary large.  You should specify at least one hidden layer.\n",
    "        \n",
    "        input_layer_size = len(self.train_set['x'][0]) \n",
    "        output_layer_size = self.nb_classes\n",
    "\n",
    "        # Static variable holding the data provided (input and related labels)\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_layer_size], name='input')\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, output_layer_size], name = 'label')\n",
    "        \n",
    "        self.hidden_layers = hidden_layers \n",
    "\n",
    "        # Each of this list keep track of the variable used by the model in order to do further analysis of the model\n",
    "        self.weights = []\n",
    "        self.layers = []\n",
    "        self.bias = []\n",
    "        \n",
    "        # Creation of the model\n",
    "        #\n",
    "        # Each layer is densely connected with the previous and next one.\n",
    "        # Moreover, a bias is added to each node (if necessary) each layer has a sigmoïd as activation function \n",
    "        #\n",
    "        \n",
    "        \n",
    "        # Creation of the first hidden layer.\n",
    "        W = tf.Variable(tf.random_normal([input_layer_size, self.hidden_layers[0]], stddev=0.01))\n",
    "        self.weights.append(W) \n",
    "        b = tf.Variable(tf.zeros([self.hidden_layers[0]]))\n",
    "        self.bias.append(b)\n",
    "        y = tf.nn.sigmoid(tf.matmul(self.x, W)+b)\n",
    "        self.layers.append(y)\n",
    "        \n",
    "        # Loop creating all the other layers\n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            W = tf.Variable(tf.random_normal([self.hidden_layers[i], self.hidden_layers[i+1]], stddev=0.1))\n",
    "            self.weights.append(W) \n",
    "            b = tf.Variable(tf.zeros([self.hidden_layers[i+1]]))\n",
    "            self.bias.append(b)\n",
    "            y = tf.nn.sigmoid(tf.matmul(self.layers[i], W)+b)\n",
    "            self.layers.append(y)\n",
    "\n",
    "        # Connection of the last hidden layer to the output layer\n",
    "        # To created a distribution of probabilies at the end, a softmax function is used (computed along with the \n",
    "        # loss function for numerical stability reasons)\n",
    "        W = tf.Variable(tf.random_normal([self.hidden_layers[-1], output_layer_size], stddev=0.1))\n",
    "        self.weights.append(W)\n",
    "        b = tf.Variable(tf.zeros([output_layer_size]))\n",
    "        self.bias.append(b)\n",
    "        self.y = tf.matmul(self.layers[-1], W)+b\n",
    "\n",
    "        # Loss function : Cross-entropy function \n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "        # Training accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        self.training_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Optimizer : Adam optimize gives us better results than a classical Gradient Method\n",
    "        self.train_step = tf.train.AdamOptimizer().minimize(self.loss, global_step = self.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the *.xlsx files and create a training and a test sets (csv format)\n",
    "#\n",
    "# Launch this only once !\n",
    "import generate_files\n",
    "\n",
    "generate_files.create_training_test_set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_input(train_set_raw, test_set_raw):\n",
    "    train_set_features , word_dict = generate_bow(train_set_raw['x'], train_set_raw['label'], False, False, False)\n",
    "    test_set_features = create_bow_by_dict(test_set_raw['x'], word_dict, True)\n",
    "\n",
    "\n",
    "    train_set = {'x' : train_set_features, 'label' : [np.array([int(s == 'Trump'),1-int(s == 'Trump')]) for s in train_set_raw['label']]}\n",
    "    test_set = {'x' : test_set_features, 'label' : [np.array([int(s == 'Trump'),1-int(s == 'Trump')]) for s in test_set_raw['label']]}\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step :\n",
      "Step 5900 (0.37171742484 mean loss))\n",
      "accuracy : 0.807422\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-7bbee1de4e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\naccuracy : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# Close the Tensorflow session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#DNN.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Etudes/Q7/AIDS/project/deep_learning_LINMA2472/classifier.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# self.train_step = tf.train.AdamOptimizer().minimize(self.loss, global_step = self.global_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "# Read the training and test sets\n",
    "train_set_raw = read_files('training.csv')\n",
    "test_set_raw = read_files('test.csv')\n",
    "\n",
    "\n",
    "# Processing of the train_set and test_set to transform them into a bag of words\n",
    "train_set, test_set = process_input(train_set_raw, test_set_raw)\n",
    "\n",
    "# Reset the Tensorflow graph at each run (to avoid creating a lot of Variables ...)\n",
    "tf.reset_default_graph() \n",
    "\n",
    "\n",
    "# Creation of the model\n",
    "DNN = PerceptronClassifier(train_set, test_set, 2, name='6_layers_perceptron')\n",
    "DNN.create_model([16, 16, 16, 16])\n",
    "\n",
    "# Create a Tensorflow session to train our network and test it\n",
    "DNN.run()\n",
    "\n",
    "# Train the network on 10 epochsii\n",
    "DNN.train(10)\n",
    "\n",
    "print('\\naccuracy : ' + str(DNN.test()))\n",
    "print(DNN.evaluate({'x': test_set['x'][0], 'label' : test_set['label'][0]}))\n",
    "# Close the Tensorflow session\n",
    "#DNN.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8d3f2ffd27c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Etudes/Q7/AIDS/project/deep_learning_LINMA2472/classifier.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# self.train_step = tf.train.AdamOptimizer().minimize(self.loss, global_step = self.global_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "print(DNN.evaluate({'x': test_set['x'][0], 'label' : test_set['label'][0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we test different deep-learning architectures with the library Keras.\n",
    "\n",
    "The tested architectures are convolutionnals one based on this article \"Convolutional Neural Networks for Sentence Classification [2014]\" by Yoon Kim. Where a precomputed embedding of the words done by Word2Vec on google-news data is used.\n",
    "\n",
    "We also test a two stacked LSTM architecture, still using the word embeddings.\n",
    "\n",
    "The plots of the different models by Keras can be found in the \"model#.png\" files. They are also shown in the notebook.\n",
    "\n",
    "To run this part you will need : \n",
    "\n",
    "- numpy\n",
    "- For the embedding :\n",
    "  - gensim\n",
    "  - Word2Vec google-news embedding, it can be found here https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit \n",
    "\n",
    "- For model formulation and optimization:\n",
    "  - keras\n",
    "\n",
    "- For model visualization, given in the \"model#.png\" files (optionnal decomment \"plot_model(.)\" call if you need it):\n",
    "  - pydot\n",
    "  - graphviz (apt-get graphviz, not the anaconda package)\n",
    "  - a lot of ram (16go recommended)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import gc\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "if not 'embeddingModel' in vars():\n",
    "    embeddingModel = 0\n",
    "    gc.collect()\n",
    "    embeddingModel = KeyedVectors.load_word2vec_format(os.environ['HOME']+'/Documents/Word2Vec_embedding/GoogleNews-vectors-negative300.bin', binary=True)#, norm_only=True)\n",
    "\n",
    "def embedding(tweet):\n",
    "    \"\"\" convert a tweet to a matrix\n",
    "        with the embedding from Word2Vec to GoogleNews\n",
    "    \"\"\"\n",
    "    E = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if word in embeddingModel:\n",
    "            E.append(embeddingModel[word])\n",
    "    \n",
    "    return np.array(E)\n",
    "        \n",
    "\n",
    "def create_dataset(filename):\n",
    "    training_list = []\n",
    "    label_list = []\n",
    "    file = open(filename, \"r\")\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    for tweet, author in reader:\n",
    "        E = embedding(tweet)\n",
    "        if not E.size<3*300:\n",
    "            training_list.append(E)\n",
    "            label_list.append(int(author=='Trump'))\n",
    "    file.close()\n",
    "\n",
    "    return {'x': training_list, 'label': label_list}\n",
    "\n",
    "Train_dataset = create_dataset('training.csv')\n",
    "x_train = Train_dataset['x']\n",
    "y_train = Train_dataset['label']\n",
    "\n",
    "Test_dataset = create_dataset('test.csv')\n",
    "x_test = Test_dataset['x']\n",
    "y_test = Test_dataset['label']\n",
    "\n",
    "#what is the length of the maximal sequence of words (for padding)\n",
    "seq_length = max(max([x.shape[0] for x in x_train]), max([x.shape[0] for x in x_test]))\n",
    "\n",
    "def zero_padding(X):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = np.vstack((X[i], np.zeros((seq_length-X[i].shape[0],300))))\n",
    "\n",
    "zero_padding(x_train)\n",
    "zero_padding(x_test)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model\n",
    "A first simpler implementation of the one given in the article. With only one convolutionnal kernel size (3) with 128 features, a global max pooling layer and a fully connected layer to the one node output.\n",
    "\n",
    "![Model plot](model1.png)\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(128, 3, activation='relu', input_shape=(seq_length,300), name=\"Convolution\"))\n",
    "model.add(GlobalMaxPooling1D(name=\"Pooling\"))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model1.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a convolutionnal layer with different kernel sizes\n",
    "A component of the two following models. Implement one convolutionnal layer with three kernel sizes (3,4,5) 100 features each and global max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_length,300), name=\"Convolution_Input\")\n",
    "convs = []\n",
    "#1\n",
    "conv = Conv1D(100, 3, activation='relu', name=\"Convolution_Ker_Size3\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling1\")(conv)\n",
    "convs.append(pool)\n",
    "#2\n",
    "conv = Conv1D(100, 4, activation='relu', name=\"Convolution_Ker_Size4\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling2\")(conv)\n",
    "convs.append(pool)\n",
    "#3\n",
    "conv = Conv1D(100, 5, activation='relu', name=\"Convolution_Ker_Size5\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling3\")(conv)\n",
    "convs.append(pool)\n",
    "out = Concatenate(name=\"Merge\")(convs)\n",
    "\n",
    "conv_model = Model(inputs=inp, outputs=out)\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model\n",
    "Close to the model presented in the article. The three kernel size for the convolutionnal layer, Dropout on the hidden layer with p=0.5, and a l2 loss on the last matrix weights (l2 constraint in the article).\n",
    "\n",
    "![Model plot](model2.png)\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(conv_model)\n",
    "model.add(Dropout(0.5, name=\"Dropout\"))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01), name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model2.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third model\n",
    "A 20 nodes fully connected intermediate layer is added before the output.\n",
    "\n",
    "![Model plot](model3.png)\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(conv_model)\n",
    "model.add(Dropout(0.5, name=\"Dropout\"))\n",
    "model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.01), name=\"Intermediate_Dense\"))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01), name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model3.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth model\n",
    "Two stacked LSTM.\n",
    "The first as a 64 dimensionnal state and return it at each time step (word). The second as a 32 dimensionnal state and only return it at the end. This last state is then used to compute the output using a dense layer.\n",
    "\n",
    "![Model plot](model4.png)\n",
    "\n",
    "Observed test set accuracy : ~90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,input_shape=(seq_length,300), name=\"First_Stacked_LSTM\"))\n",
    "model.add(LSTM(32, name=\"Second_Stacked_LSTM\"))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=20, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model4.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "There is no big difference of performance between the models. Training time where roughly the same ~1-2min.\n",
    "\n",
    "Interestingly the accuracy on the training set where usually far better than the accuracy on the test set for the convolutionnal models. But this was not observed for the two stacked LSTM. It seems that the convolutionnal models where able to fit some examples from the training set without making generalization improvements or decreases in performance. While the stacked LSTM had not this expressive possibility, such that the generalization property was stronger. If a model is less expressive, as measured by the VC-dimension by example, the generalization is stronger for better or worse.\n",
    "\n",
    "More than 90% accuracy seems acceptable since the model works on the semantic of the words used, rather than the syntax due to the embedding (assuming the embedding reflects the semantic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Tensorflow website](https://www.tensorflow.org/api_docs/)\n",
    "2. [Stanford Tensorflow course notes](http://web.stanford.edu/class/cs20si/)\n",
    "3. [Kaggle tutorial on Bag of words](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words)\n",
    "3. \"Convolutional Neural Networks for Sentence Classification [2014]\" by Yoon Kim.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
