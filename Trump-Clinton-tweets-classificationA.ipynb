{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LINMA 2472 : Algorithms in Data Science**\n",
    "===============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project on deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a common task in machine learning.  In this project, we will tackle the task of classifying tweets of the two presidential candidates for the 2017 election.  To do so, we will use a database of about 20k tweets of both candidates.  For Donald Trump, we have all the tweets he posted from 05/04/2009 to 11/26/2017 while we have for Hillary Clinton her tweets from 06/10/2013 to 11/24/2017.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project, we decided to use the python version of the library [TensorFlow](https://www.tensorflow.org/).  Since this library is quite powerful, we decided not to use its high level tools as a black box and code ourselves our classifier as much as possible.  One could argue that we could develop our classifier without the help of any library, but we could not have the same results for sure.  Backpropagation may be quite tricky to implement and coding a fancier optimization method than Gradient method would have been out of range considering the time resources we had.  Moreover, TensorFlow provides us great tools to track the performance of our classifiers as the TensorBoard tool.  In this first part, we implemented a simple perceptron model with a naïve feature extraction of the texts : a bag of word representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used Keras. Keras is a higher order library based on one lower level library, like TensorFlow or CNTK for example. We used it to easily test several models, convolutionnals and recurrent.  Indeed, we wanted to use more sophisticated models and implementing them would have been quite tedious.  Using Keras, we discovered another trending library to do deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developped a python abstraction for classifiers you can find in classifier.py.  In that way, it is really easy to build another classifier based on another model, you just have to redefine the method create_model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you need to lauch this part of the project : \n",
    "\n",
    " - numpy\n",
    " - nltk (For the bag of words processing)\n",
    " - [Tensorflow](https://www.tensorflow.org/install/)\n",
    "\n",
    "You can track the progress of the training by launching tensorboard in your terminal :\n",
    "`tensorboard --logdirs='.graphs/'`\n",
    "Then, open the link provided in your web browser to use the tools of tensorboard.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/brieuc/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import csv\n",
    "\n",
    "# Our custom libraries\n",
    "from nlp_utils import create_bow_by_dict, create_words_dict\n",
    "from utils import read_files\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classifier model : The perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PerceptronClassifier(Classifier):\n",
    "    # This class inherit from the the class Classifier, an abstraction dealing with the run of the flows, the saving of \n",
    "    # the  intermediate results, the training step, the testing step, ...\n",
    "    # See classifier.py for more informations\n",
    "    \n",
    "    def create_model(self, hidden_layers):\n",
    "        # Create the structure of the perceptron.\n",
    "        # hidden_layers should be a list containing the number of nodes for each layer.\n",
    "        # Each layer is densely connected with the previous and next one.\n",
    "        # The number of layer can be arbitrary large.  You should specify at least one hidden layer.\n",
    "        \n",
    "        input_layer_size = len(self.train_set['x'][0]) \n",
    "        output_layer_size = self.nb_classes\n",
    "\n",
    "        # Static variable holding the data provided (input and related labels)\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_layer_size], name='input')\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, output_layer_size], name = 'label')\n",
    "        \n",
    "        self.hidden_layers = hidden_layers \n",
    "\n",
    "        # Each of this list keep track of the variable used by the model in order to do further analysis of the model\n",
    "        self.weights = []\n",
    "        self.layers = []\n",
    "        self.bias = []\n",
    "        \n",
    "        # Creation of the model\n",
    "        #\n",
    "        # Each layer is densely connected with the previous and next one.\n",
    "        # Moreover, a bias is added to each node (if necessary) each layer has a sigmoïd as activation function \n",
    "        #\n",
    "        \n",
    "        # Creation of the first hidden layer.\n",
    "        W = tf.Variable(tf.random_normal([input_layer_size, self.hidden_layers[0]], stddev=0.01))\n",
    "        self.weights.append(W) \n",
    "        b = tf.Variable(tf.zeros([self.hidden_layers[0]]))\n",
    "        self.bias.append(b)\n",
    "        y = tf.nn.sigmoid(tf.matmul(self.x, W)+b)\n",
    "        self.layers.append(y)\n",
    "        \n",
    "        # Loop creating all the other layers\n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            W = tf.Variable(tf.random_normal([self.hidden_layers[i], self.hidden_layers[i+1]], stddev=0.1))\n",
    "            self.weights.append(W) \n",
    "            b = tf.Variable(tf.zeros([self.hidden_layers[i+1]]))\n",
    "            self.bias.append(b)\n",
    "            y = tf.nn.sigmoid(tf.matmul(self.layers[i], W)+b)\n",
    "            self.layers.append(y)\n",
    "\n",
    "        # Connection of the last hidden layer to the output layer\n",
    "        # To created a distribution of probabilies at the end, a softmax function is used (computed along with the \n",
    "        # loss function for numerical stability reasons)\n",
    "        W = tf.Variable(tf.random_normal([self.hidden_layers[-1], output_layer_size], stddev=0.1))\n",
    "        self.weights.append(W)\n",
    "        b = tf.Variable(tf.zeros([output_layer_size]))\n",
    "        self.bias.append(b)\n",
    "        self.y = tf.matmul(self.layers[-1], W)+b\n",
    "\n",
    "        # Loss function : Cross-entropy function \n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "        # Training accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        self.training_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Optimizer : Adam optimize gives us better results than a classical Gradient Method\n",
    "        self.train_step = tf.train.AdamOptimizer().minimize(self.loss, global_step = self.global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the *.xlsx files and create a training and a test sets (csv format)\n",
    "#\n",
    "# Launch this only once !\n",
    "import generate_files\n",
    "\n",
    "generate_files.create_training_test_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform our tweets into features our classifier can use, we use a bag of words (abbreviated bow).  The key concept is quite simple : from all the tweets in our training set, we create a big dictionary mapping words to indices (and vice-versa.  Then, for each tweet, we create an array that counts for each index the number of time the word mapped to this index appears in the tweet : this array is called the bag of words.  Finally, we can use this array as the array of features of each tweet.\n",
    "\n",
    "Example  :\n",
    "\n",
    "\"Make America great again\"\n",
    "\"Vote for me, vote for America\"\n",
    "\"Fake news\"\n",
    "\n",
    "=> word dictionary : {'Make' : 1, 'America' : 2, 'great': 3, 'again' : 4, 'Vote': 5, 'for' : 6, 'me': 7, 'Fake' : 8, 'news' : 9}\n",
    "\n",
    "Bag of words for \"Vote for me, vote for America\" : \\[0,1,0,0,2,2,1,0,0\\]\n",
    "\n",
    "One can remark that using this method, we don't use the order of the words in the sentence.  Thus, 'The cat eats the dog' and 'the dog eats the cat' have the same bag of words, while being very different semantically.  Nevertheless, while being very naïve, we managed to get some results with this method.\n",
    "\n",
    "Finally, the bag of words can be very large, so large that it is not feasible to have as many features.  Several issues can arise : very long  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords** : stopwords are very common words : \"the\", \"and\", \"or\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_input(train_set_raw, test_set_raw, tf_IDF_option = True, stem_option= True, stopwords_option = True):\n",
    "    \n",
    "    words_dict = create_words_dict(train_set_raw['x'], train_set_raw['label'], tf_IDF_option, stem_option, stopwords_option)\n",
    "    train_set_features = create_bow_by_dict(train_set_raw['x'], words_dict, tf_IDF_option, stem_option, stopwords_option)\n",
    "    test_set_features  = create_bow_by_dict(test_set_raw['x'], words_dict, tf_IDF_option, stem_option, stopwords_option)\n",
    "\n",
    "\n",
    "    train_set = {'x' : train_set_features, 'label' : [np.array([int(s == 'Trump'),1-int(s == 'Trump')]) for s in train_set_raw['label']]}\n",
    "    test_set = {'x' : test_set_features, 'label' : [np.array([int(s == 'Trump'),1-int(s == 'Trump')]) for s in test_set_raw['label']]}\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and test of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the training and test sets\n",
    "train_set_raw = read_files('training.csv')\n",
    "test_set_raw = read_files('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is shown the dependance of the accuracy with respect to different options on the treatment of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step :\n",
      "Step 5900 (0.276405125707 mean loss)\n",
      "accuracy : 0.869609\n"
     ]
    }
   ],
   "source": [
    "# Processing of the train_set and test_set to transform them into a bag of words\n",
    "train_set, test_set = process_input(train_set_raw, test_set_raw, tf_IDF_option = False, stem_option= False, stopwords_option = False)\n",
    "    \n",
    "# Reset the Tensorflow graph at each run (to avoid creating a lot of Variables ...)\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# Creation of the model\n",
    "DNN = PerceptronClassifier(train_set, test_set, 2, name='6_layers_perceptron')\n",
    "DNN.create_model([16, 16, 16, 16])\n",
    "\n",
    "# Create a Tensorflow session to train our network and test it\n",
    "DNN.run()\n",
    "\n",
    "# Train the network on 10 epochs\n",
    "DNN.train(10)\n",
    "\n",
    "print('\\naccuracy : ' + str(DNN.test()))\n",
    "\n",
    "# Close the Tensorflow session\n",
    "DNN.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step :\n",
      "Step 5900 (0.262035298496 mean loss)\n",
      "accuracy : 0.873621\n"
     ]
    }
   ],
   "source": [
    "# Processing of the train_set and test_set to transform them into a bag of words\n",
    "train_set, test_set = process_input(train_set_raw, test_set_raw, tf_IDF_option = False, stem_option= False, stopwords_option = True)\n",
    "    \n",
    "# Reset the Tensorflow graph at each run (to avoid creating a lot of Variables ...)\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# Creation of the model\n",
    "DNN = PerceptronClassifier(train_set, test_set, 2, name='6_layers_perceptron')\n",
    "DNN.create_model([16, 16, 16, 16])\n",
    "\n",
    "# Create a Tensorflow session to train our network and test it\n",
    "DNN.run()\n",
    "\n",
    "# Train the network on 10 epochs\n",
    "DNN.train(10)\n",
    "\n",
    "print('\\naccuracy : ' + str(DNN.test()))\n",
    "\n",
    "# Close the Tensorflow session\n",
    "DNN.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step :\n",
      "Step 5900 (0.264278751537 mean loss)\n",
      "accuracy : 0.877633\n"
     ]
    }
   ],
   "source": [
    "# Processing of the train_set and test_set to transform them into a bag of words\n",
    "train_set, test_set = process_input(train_set_raw, test_set_raw, tf_IDF_option = False, stem_option= True, stopwords_option = True)\n",
    "    \n",
    "# Reset the Tensorflow graph at each run (to avoid creating a lot of Variables ...)\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# Creation of the model\n",
    "DNN = PerceptronClassifier(train_set, test_set, 2, name='6_layers_perceptron')\n",
    "DNN.create_model([16, 16, 16, 16])\n",
    "\n",
    "# Create a Tensorflow session to train our network and test it\n",
    "DNN.run()\n",
    "\n",
    "# Train the network on 10 epochs\n",
    "DNN.train(10)\n",
    "\n",
    "print('\\naccuracy : ' + str(DNN.test()))\n",
    "\n",
    "# Close the Tensorflow session\n",
    "DNN.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step :\n",
      "Step 5900 (0.272299580872 mean loss)\n",
      "accuracy : 0.875627\n"
     ]
    }
   ],
   "source": [
    "# Processing of the train_set and test_set to transform them into a bag of words\n",
    "train_set, test_set = process_input(train_set_raw, test_set_raw, tf_IDF_option = True, stem_option= True, stopwords_option = True)\n",
    "    \n",
    "# Reset the Tensorflow graph at each run (to avoid creating a lot of Variables ...)\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# Creation of the model\n",
    "DNN = PerceptronClassifier(train_set, test_set, 2, name='6_layers_perceptron')\n",
    "DNN.create_model([16, 16, 16, 16])\n",
    "\n",
    "# Create a Tensorflow session to train our network and test it\n",
    "DNN.run()\n",
    "\n",
    "# Train the network on 10 epochs\n",
    "DNN.train(10)\n",
    "\n",
    "print('\\naccuracy : ' + str(DNN.test()))\n",
    "\n",
    "# Close the Tensorflow session\n",
    "DNN.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we test different deep-learning architectures with the library Keras.\n",
    "\n",
    "The tested architectures are convolutionnals one based on this article \"Convolutional Neural Networks for Sentence Classification [2014]\" by Yoon Kim. Where a precomputed embedding of the words done by Word2Vec on google-news data is used.\n",
    "\n",
    "We also test a two stacked LSTM architecture, still using the word embeddings.\n",
    "\n",
    "The plots of the different models by Keras can be found in the \"model#.png\" files. They are also shown in the notebook.\n",
    "\n",
    "To run this part you will need : \n",
    "\n",
    "- numpy\n",
    "- For the embedding :\n",
    "  - gensim\n",
    "  - Word2Vec google-news embedding, it can be found here https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit \n",
    "\n",
    "- For model formulation and optimization:\n",
    "  - keras\n",
    "\n",
    "- For model visualization, given in the \"model#.png\" files (optionnal decomment \"plot_model(.)\" call if you need it):\n",
    "  - pydot\n",
    "  - graphviz (apt-get graphviz, not the anaconda package)\n",
    "  - a lot of ram (16go recommended)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import gc\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "if not 'embeddingModel' in vars():\n",
    "    embeddingModel = 0\n",
    "    gc.collect()\n",
    "    embeddingModel = KeyedVectors.load_word2vec_format(os.environ['HOME']+'/Documents/Word2Vec_embedding/GoogleNews-vectors-negative300.bin', binary=True)#, norm_only=True)\n",
    "\n",
    "def embedding(tweet):\n",
    "    \"\"\" convert a tweet to a matrix\n",
    "        with the embedding from Word2Vec to GoogleNews\n",
    "    \"\"\"\n",
    "    E = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if word in embeddingModel:\n",
    "            E.append(embeddingModel[word])\n",
    "    \n",
    "    return np.array(E)\n",
    "        \n",
    "\n",
    "def create_dataset(filename):\n",
    "    training_list = []\n",
    "    label_list = []\n",
    "    file = open(filename, \"r\")\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    for tweet, author in reader:\n",
    "        E = embedding(tweet)\n",
    "        if not E.size<3*300:\n",
    "            training_list.append(E)\n",
    "            label_list.append(int(author=='Trump'))\n",
    "    file.close()\n",
    "\n",
    "    return {'x': training_list, 'label': label_list}\n",
    "\n",
    "Train_dataset = create_dataset('training.csv')\n",
    "x_train = Train_dataset['x']\n",
    "y_train = Train_dataset['label']\n",
    "\n",
    "Test_dataset = create_dataset('test.csv')\n",
    "x_test = Test_dataset['x']\n",
    "y_test = Test_dataset['label']\n",
    "\n",
    "#what is the length of the maximal sequence of words (for padding)\n",
    "seq_length = max(max([x.shape[0] for x in x_train]), max([x.shape[0] for x in x_test]))\n",
    "\n",
    "def zero_padding(X):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = np.vstack((X[i], np.zeros((seq_length-X[i].shape[0],300))))\n",
    "\n",
    "zero_padding(x_train)\n",
    "zero_padding(x_test)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model\n",
    "A first simpler implementation of the one given in the article. With only one convolutionnal kernel size (3) with 128 features, a global max pooling layer and a fully connected layer to the one node output.\n",
    "\n",
    "![Model plot](model1.png)\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(128, 3, activation='relu', input_shape=(seq_length,300), name=\"Convolution\"))\n",
    "model.add(GlobalMaxPooling1D(name=\"Pooling\"))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model1.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a convolutionnal layer with different kernel sizes\n",
    "A component of the two following models. Implement one convolutionnal layer with three kernel sizes (3,4,5) 100 features each and global max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_length,300), name=\"Convolution_Input\")\n",
    "convs = []\n",
    "#1\n",
    "conv = Conv1D(100, 3, activation='relu', name=\"Convolution_Ker_Size3\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling1\")(conv)\n",
    "convs.append(pool)\n",
    "#2\n",
    "conv = Conv1D(100, 4, activation='relu', name=\"Convolution_Ker_Size4\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling2\")(conv)\n",
    "convs.append(pool)\n",
    "#3\n",
    "conv = Conv1D(100, 5, activation='relu', name=\"Convolution_Ker_Size5\")(inp)\n",
    "pool = GlobalMaxPooling1D(name=\"Global_Pooling3\")(conv)\n",
    "convs.append(pool)\n",
    "out = Concatenate(name=\"Merge\")(convs)\n",
    "\n",
    "conv_model = Model(inputs=inp, outputs=out)\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model\n",
    "Close to the model presented in the article. The three kernel size for the convolutionnal layer, Dropout on the hidden layer with p=0.5, and a l2 loss on the last matrix weights (l2 constraint in the article).\n",
    "\n",
    "![Model plot](model2.png)\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(conv_model)\n",
    "model.add(Dropout(0.5, name=\"Dropout\"))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01), name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model2.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third model\n",
    "A 20 nodes fully connected intermediate layer is added before the output.\n",
    "\n",
    "![Model plot](model3.png)\n",
    "\n",
    "Observed test set accuracy : 92-93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(conv_model)\n",
    "model.add(Dropout(0.5, name=\"Dropout\"))\n",
    "model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.01), name=\"Intermediate_Dense\"))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01), name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model3.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth model\n",
    "Two stacked LSTM.\n",
    "The first as a 64 dimensionnal state and return it at each time step (word). The second as a 32 dimensionnal state and only return it at the end. This last state is then used to compute the output using a dense layer.\n",
    "\n",
    "![Model plot](model4.png)\n",
    "\n",
    "Observed test set accuracy : ~90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,input_shape=(seq_length,300), name=\"First_Stacked_LSTM\"))\n",
    "model.add(LSTM(32, name=\"Second_Stacked_LSTM\"))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n",
    "model.summary()\n",
    "\n",
    "#loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#optimization with early stopping\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=20, callbacks=[earlyStopping], \n",
    "          validation_split=0.1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test,y_test, batch_size=64)\n",
    "\n",
    "#display accuracy and plot model\n",
    "print(\"\\nAccuracy on the test set : \"+str(score[1])+\"\\n\\n\")\n",
    "#plot_model(model, to_file=\"model4.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "There is no big difference of performance between the models. Training time where roughly the same ~1-2min.\n",
    "\n",
    "Interestingly the accuracy on the training set where usually far better than the accuracy on the test set for the convolutionnal models. But this was not observed for the two stacked LSTM. It seems that the convolutionnal models where able to fit some examples from the training set without making generalization improvements or decreases in performance. While the stacked LSTM had not this expressive possibility, such that the generalization property was stronger. If a model is less expressive, as measured by the VC-dimension by example, the generalization is stronger for better or worse.\n",
    "\n",
    "More than 90% accuracy seems acceptable since the model works on the semantic of the words used, rather than the syntax due to the embedding (assuming the embedding reflects the semantic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Tensorflow website](https://www.tensorflow.org/api_docs/)\n",
    "2. [Stanford Tensorflow course notes](http://web.stanford.edu/class/cs20si/)\n",
    "3. [Kaggle tutorial on Bag of words](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words)\n",
    "3. \"Convolutional Neural Networks for Sentence Classification [2014]\" by Yoon Kim.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
