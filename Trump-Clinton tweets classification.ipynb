{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hdev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "from simple_classifier import generate_bow, create_bow_by_dict\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    training_list = []\n",
    "    label_list = []\n",
    "    file = open(filename, \"r\")\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    for tweet, author in reader:\n",
    "        training_list.append(tweet)\n",
    "        label_list.append(author)\n",
    "    file.close()\n",
    "    \n",
    "    return {'x' : training_list, 'label' : label_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    source : https://github.com/dennybritz/cnn-text-classification-tf/blob/master/data_helpers.py\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    # Reference : \n",
    "    # https://www.tensorflow.org/get_started/mnist/beginners\n",
    "    # http://web.stanford.edu/class/cs20si/lectures/notes_05.pdf\n",
    " \n",
    "    def __init__(self, train_set, test_set, nb_classes, name = 'DNN'):\n",
    "        \n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.nb_classes = nb_classes\n",
    "        \n",
    "        self.is_running = False\n",
    "        self.model_name = name\n",
    "\n",
    "        \n",
    "        # TF saver to save regularly our progress\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name = 'global_step')\n",
    "        \n",
    "        self.saver =  tf.train.Saver()\n",
    "\n",
    "    def create_model(self, hidden_layers):\n",
    "        # At least one hidden layer !\n",
    "        # Create the structure of the deep neural network\n",
    "       \n",
    "        input_layer_size = len(self.train_set['x'][0]) \n",
    "        output_layer_size = self.nb_classes\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_layer_size], name='input')\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, output_layer_size], name = 'label')\n",
    "        \n",
    "        self.hidden_layers = hidden_layers \n",
    "\n",
    "        self.weights = []\n",
    "        self.layers = []\n",
    "        self.bias = []\n",
    "        \n",
    "        W = tf.Variable(tf.random_normal([input_layer_size, self.hidden_layers[0]], stddev=0.01))\n",
    "        self.weights.append(W) \n",
    "        b = tf.Variable(tf.zeros([self.hidden_layers[0]]))\n",
    "        self.bias.append(b)\n",
    "        y = tf.nn.sigmoid(tf.matmul(self.x, W)+b)\n",
    "        self.layers.append(y)\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            W = tf.Variable(tf.random_normal([self.hidden_layers[i], self.hidden_layers[i+1]], stddev=0.1))\n",
    "            self.weights.append(W) \n",
    "            b = tf.Variable(tf.zeros([self.hidden_layers[i+1]]))\n",
    "            self.bias.append(b)\n",
    "            y = tf.nn.sigmoid(tf.matmul(self.layers[i], W)+b)\n",
    "            self.layers.append(y)\n",
    "\n",
    "        W = tf.Variable(tf.random_normal([self.hidden_layers[-1], output_layer_size], stddev=0.1))\n",
    "        self.weights.append(W)\n",
    "        b = tf.Variable(tf.zeros([output_layer_size]))\n",
    "        self.bias.append(b)\n",
    "        \n",
    "        self.y = tf.matmul(self.layers[-1], W)+b\n",
    "\n",
    "        # Loss function\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "        # Training accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        self.training_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Optimizer\n",
    "        self.train_step = tf.train.AdamOptimizer().minimize(self.loss, global_step = self.global_step)\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        if not self.is_running:\n",
    "            self.run()\n",
    "        n_epoch = 10\n",
    " \n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(self.train_set['x'], self.train_set['label'])), 32, n_epoch)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        step = 0\n",
    "        total_loss = 0\n",
    "        losses = []\n",
    "\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            loss_batch, _, summary = self.sess.run([self.loss, self.train_step, self.summary_op], feed_dict={self.x : list(x_batch), self.y_ : list(y_batch)})\n",
    "            self.writer.add_summary(summary, global_step = step)\n",
    "            total_loss += loss_batch\n",
    "            #print(loss_batch)\n",
    "            if step % 100 == 0 :\n",
    "                losses.append(total_loss)\n",
    "                print('Step ' + str(step) + ' (' + str(total_loss /100) + ' mean loss)', end='\\r')\n",
    "                total_loss = 0\n",
    "                self.saver.save(self.sess, 'checkpoints/'+self.model_name, global_step=self.global_step) \n",
    "\n",
    "            step += 1\n",
    "        \n",
    "\n",
    "    def evaluate(self, x):\n",
    "        if not self.is_running:\n",
    "            self.run()\n",
    "\n",
    "        return self.sess.run(tf.argmax(self.y, 1), feed_dict= {self.x: x})  \n",
    "\n",
    "\n",
    "    def test(self):   \n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        return self.sess.run(accuracy, feed_dict={self.x: self.test_set['x'], self.y_ : self.test_set['label']}) \n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        if os.path.exists('.graphs'):  \n",
    "            shutil.rmtree('.graphs')\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter('.graphs', self.sess.graph)    \n",
    "        self.create_summaries()\n",
    "\n",
    "        self.is_running = True\n",
    "        tf.global_variables_initializer().run()\n",
    "    \n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "        self.sess.close()\n",
    "        self.is_running = False\n",
    "\n",
    "    def create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def restore(self):\n",
    "        if not self.is_running:\n",
    "            self.run()\n",
    "        #saver = tf.train.import_meta_graph('checkpoints/2_layers_perceptron-9001.meta')\n",
    "        #saver.restore(self.sess,tf.train.latest_checkpoint('.'))\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            print(ckpt.model_checkpoint_path) \n",
    "            tf.global_variables_initializer().run() \n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "            print(DNN.sess.run(DNN.bias[0], feed_dict= {DNN.x: [test_set['x'][0]]}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_raw = read_files('training.csv')\n",
    "test_set_raw = read_files('test.csv')\n",
    "\n",
    "\n",
    "train_set_features , word_dict = generate_bow(train_set_raw['x'], train_set_raw['label'], True)\n",
    "test_set_features = create_bow_by_dict(test_set_raw['x'], word_dict, True)\n",
    "\n",
    "\n",
    "train_set = {'x' : train_set_features, 'label' : [np.array([float(s == 'Trump'),1-int(s == 'Trump')]) for s in train_set_raw['label']]}\n",
    "test_set = {'x' : test_set_features, 'label' : [np.array([float(s == 'Trump'),1-int(s == 'Trump')]) for s in test_set_raw['label']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "accuracy : 0.87562730474 mean loss))\n"
     ]
    }
   ],
   "source": [
    "# example : \n",
    "tf.reset_default_graph() \n",
    "\n",
    "DNN = DeepNeuralNetwork(train_set, test_set, 2, name='2_layers_perceptron')\n",
    "DNN.create_model([16, 16, 16, 16])\n",
    "DNN.run()\n",
    "print(DNN.sess.run(DNN.bias[0], feed_dict= {DNN.x: [test_set['x'][0]]}))\n",
    "DNN.train()\n",
    "#DNN.restore()\n",
    "print('accuracy : ' + str(DNN.test()))\n",
    "DNN.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
