{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow\nModuleNotFoundError: No module named '_pywrap_tensorflow'\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named '_pywrap_tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# use `dlopen()` for dynamic loading.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-41389fad42b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mplease\u001b[0m \u001b[0mexit\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0msource\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrelaunch\u001b[0m \u001b[0myour\u001b[0m \u001b[0mpython\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m from there.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 60\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"C:\\Users\\henri\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow\nModuleNotFoundError: No module named '_pywrap_tensorflow'\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_bag_of_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cd59a243de78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msimple_classifier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_bag_of_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'create_bag_of_words'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "from simple_classifier import generate_bow, create_bag_of_words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    training_list = []\n",
    "    label_list = []\n",
    "    file = open(filename, \"r\")\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    for tweet, author in reader:\n",
    "        training_list.append(tweet)\n",
    "        label_list.append(author)\n",
    "    file.close()\n",
    "    \n",
    "    return {'x' : training_list, 'label' : label_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    # Reference : \n",
    "    # https://www.tensorflow.org/get_started/mnist/beginners\n",
    "    # http://web.stanford.edu/class/cs20si/lectures/notes_05.pdf\n",
    " \n",
    "    def __init__(self, train_set, test_set, nb_classes, name = 'DNN'):\n",
    "        \n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.nb_classes = nb_classes\n",
    "        \n",
    "        self.is_running = False\n",
    "        self.model_name = name\n",
    "\n",
    "        \n",
    "        # TF saver to save regularly our progress\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name = 'global_step')\n",
    "        \n",
    "        self.saver =  tf.train.Saver()\n",
    "\n",
    "    def create_model(self, hidden_layers):\n",
    "        # At least one hidden layer !\n",
    "        # Create the structure of the deep neural network\n",
    "       \n",
    "        input_layer_size = len(self.train_set['x'][0]) \n",
    "        output_layer_size = self.nb_classes\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_layer_size])\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, output_layer_size])\n",
    "        \n",
    "        self.hidden_layers = hidden_layers \n",
    "\n",
    "        self.weights = []\n",
    "        self.layers = []\n",
    "        self.bias = []\n",
    "        \n",
    "        W = tf.Variable(tf.random_normal([input_layer_size, self.hidden_layers[0]], stddev=0.01))\n",
    "        self.weights.append(W) \n",
    "        b = tf.Variable(tf.zeros([self.hidden_layers[0]]))\n",
    "        self.bias.append(b)\n",
    "        y = tf.nn.sigmoid(tf.matmul(self.x, W)+b)\n",
    "        self.layers.append(y)\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            W = tf.Variable(tf.random_normal([self.hidden_layers[i], self.hidden_layers[i+1]], stddev=0.1))\n",
    "            self.weights.append(W) \n",
    "            b = tf.Variable(tf.zeros([self.hidden_layers[i+1]]))\n",
    "            self.bias.append(b)\n",
    "            y = tf.nn.sigmoid(tf.matmul(self.layers[i], W)+b)\n",
    "            self.layers.append(y)\n",
    "\n",
    "        W = tf.Variable(tf.random_normal([self.hidden_layers[-1], output_layer_size], stddev=0.1))\n",
    "        self.weights.append(W)\n",
    "        b = tf.Variable(tf.zeros([output_layer_size]))\n",
    "        self.bias.append(b)\n",
    "        \n",
    "        self.y = tf.matmul(self.layers[-1], W)+b\n",
    "\n",
    "        # Loss function\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "        # Training accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        self.training_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Optimizer\n",
    "        self.train_step = tf.train.AdamOptimizer().minimize(self.loss, global_step = self.global_step)\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        if not self.is_running:\n",
    "            self.run()\n",
    "        n_epoch = 1\n",
    " \n",
    "        \"\"\"\n",
    "        training_dataset = tf.data.Dataset.from_tensor_slices((self.train_set['x'], self.train_set['label']))    \n",
    "        training_dataset = training_dataset.repeat(n_epoch)\n",
    "        training_dataset = training_dataset.shuffle(buffer_size=10000)\n",
    "        training_dataset = training_dataset.batch(32)\n",
    "        \n",
    "        iterator = training_dataset.make_one_shot_iterator()\n",
    "\n",
    "        # self.sess.run(iterator.initializer)\n",
    "  \n",
    "        losses = []\n",
    "        total_loss = 0\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            try:\n",
    "                next_element = iterator.get_next()\n",
    "                print(next_element)\n",
    "                print(next_element[0])\n",
    "                loss_batch, _, summary = self.sess.run([self.loss, self.train_step, self.summary_op], feed_dict={self.x:next_element[0], self.y_:next_element[1]})\n",
    "                self.writer.add_summary(summary, global_step = step)\n",
    "                total_loss += loss_batch    \n",
    "                if step % 1000 == 0 :\n",
    "                    losses.append(total_loss)\n",
    "                    print(total_loss / 1000)\n",
    "                    total_loss = 0\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        \"\"\"\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(self.train_set['x'], self.train_set['label'])), 32, n_epoch)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            print(x_batch)\n",
    "            print(y_batch)\n",
    "            loss_batch, _, summary = self.sess.run([self.loss, self.train_step, self.summary_op], feed_dict={self.x : list(x_batch), self.y_ : list(y_batch)})\n",
    "            self.writer.add_summary(summary, global_step = step)\n",
    "            total_loss += loss_batch    \n",
    "            if step % 1000 == 0 :\n",
    "                losses.append(total_loss)\n",
    "                print(total_loss / 1000)\n",
    "                total_loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        for batch in batches :\n",
    "            print(batch)\n",
    "            batch_xs, batch_ys = tf.next_batch(30,  self.train_set)\n",
    "            loss_batch, _, summary = self.sess.run([self.loss, self.train_step, self.summary_op], feed_dict={self.x:batch_xs, self.y_:batch_ys})\n",
    "            self.writer.add_summary(summary, global_step = step)\n",
    "            total_loss += loss_batch    \n",
    "            if step % 1000 == 0 :\n",
    "                losses.append(total_loss)\n",
    "                print(total_loss / 1000)\n",
    "                total_loss = 0\n",
    "        \"\"\"\n",
    "            \n",
    "\n",
    "    def evaluate(self, x):\n",
    "        if not self.is_running:\n",
    "            self.run()\n",
    "\n",
    "        return self.sess.run(tf.argmax(self.y, 1), feed_dict= {self.x: x})  \n",
    "\n",
    "\n",
    "    def test(self):   \n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        return self.sess.run(accuracy, feed_dict={self.x: self.test_set['x'], self.y_ : self.test_set['label']}) \n",
    "        \n",
    "    \n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        if os.path.exists('.graphs'):  \n",
    "            shutil.rmtree('.graphs')\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter('.graphs', self.sess.graph)    \n",
    "        self.create_summaries()\n",
    "\n",
    "        self.is_running = True\n",
    "        tf.global_variables_initializer().run()\n",
    "    \n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "        self.sess.close()\n",
    "        self.is_running = False\n",
    "\n",
    "    def create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_raw = read_files('training.csv')\n",
    "test_set_raw = read_files('test.csv')\n",
    "\n",
    "\n",
    "train_set_features , word_dict = generate_bow(train_set_raw['x'], train_set_raw['label'])\n",
    "test_set_features = create_bag_of_words(test_set_raw['x'], word_dict, False)\n",
    "\n",
    "\n",
    "train_set = {'x' : train_set_features, 'label' : [np.array([float(s == 'Trump'),1-int(s == 'Trump')]) for s in train_set_raw['label']]}\n",
    "test_set = {'x' : test_set_features, 'label' : [np.array([float(s == 'Trump'),1-int(s == 'Trump')]) for s in test_set_raw['label']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example : \n",
    "\n",
    "\n",
    "DNN = DeepNeuralNetwork(train_set, test_set, 2, name='2_layers_perceptron')\n",
    "DNN.create_model([1])\n",
    "DNN.run()\n",
    "print(DNN.sess.run(DNN.bias[0], feed_dict= {DNN.x: [test_set['x'][0]]}))\n",
    "DNN.train()\n",
    "print('accuracy : ' + str(DNN.test()))\n",
    "print(DNN.evaluate(test_set['x']))\n",
    "DNN.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
